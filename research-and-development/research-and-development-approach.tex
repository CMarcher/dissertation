\chapter{Research and Development Approach}

\section{Kubernetes cluster architecture}

\section{Performance analysis experiment components}
\subsection{Controlled environment}

Difficulties were encountered obtaining consistent results when conducting ad hoc performance analysis experiments. Because of the usage of a 4G router for providing internet access to the Raspberry Pi cluster, highly variable bandwidth and latency conditions were encountered. One performance test with increasing load up to sixty requests per second might encounter exponentially increasing response times half-way during the test, while another repetition would see this occur three-quarters through the test. 

Another problem caused by relying on 4G mobile network infrastructure is the upper bandwidth limitation. The 4G router in use has (in its physical position in a lab) a peak download speed of \textasciitilde50.0 Mbit/second (\textasciitilde6.25 Mbyte/second), and an upload speed of \textasciitilde20.0 Mbit/second (\textasciitilde2.5 Mbyte/second). With increasing request rates during performance tests, a bandwidth bottleneck can be encountered, with the cluster unable to serve data because of a lack of sufficient capacity. Though perfect bandwidth and latency stability cannot be expected on the internet, mobile networks are especially prone to instability, and inhibit the ability to replicate a standard environment for applications deployed to ``the cloud'', i.e. a datacentre with high bandwidth capacity. This is exarcerbated by the usage of Cloudflare Tunnel on the cluster to enable external access, which provides a lower bound on maximum bandwidth than if the router was directly accessible via the internet.

To conduct consistent load-tests that provide a higher signal to noise ratio, a stable cloud infrastructure environment was simulated. Load-tests were performed from the ingress node, sending requests to the Kubernetes control node over the LAN (local area network) provided by the network switch. An Auckland-based data centre environment will be replicated, with an expected latency of 5 miiliseconds from an in-country client device to the data centre. This latency will be simulated via the \verb|tc| Linux command as depicted in Listings \ref{lst:pre-test-latency} and \ref{lst:post-test-latency}. The \verb|tc| command allows the traffic travelling via a particular network interface to have artificial constraints added, such as with bandwidth and latency. Prior to every test, 5 milliseconds of latency was added, and then removed after each test.

\begin{center}
    \begin{minipage}{\linewidth}
    \centering
    \begin{lstlisting}[language=bash, caption={Pre-test latency simulation command},label=lst:pre-test-latency]
    
    sudo tc qdisc add dev eth0 root netem delay 5ms
    
    \end{lstlisting}
    \end{minipage}
\end{center}

\begin{center}
    \begin{minipage}{\linewidth}
    \centering
    \begin{lstlisting}[language=bash, caption={Post-test latency simulation teardown},label=lst:post-test-latency]
    
        sudo tc qdisc del dev eth0 root netem
    
    \end{lstlisting}
    \end{minipage}
\end{center}

% ^^ fix stupid centreing and spacing later ^^

% Collect data to demonstrate the instability of external access %

% Show maximum LAN bandwidth results

% Show results of added latency perhaps.

\subsection{Controlling variables}

As with any valid experiment, any one load-test carried out assesses the performance differences observed after modifying one variable. For example, the number of replicas for a Kubernetes deployment, in one set of experiments, have been varied, while all other configuration fields have been kept the same. In some experiments, an entire block of configuration (such as the presence of a \verb|HorizontalPodAutoscaler| in a \verb|Deployment|) have been independently assessed, and everything else controlled. 

\subsection{Preliminary performance modelling}
\subsection{Empirical comparison}
\subsubsection{Grafana k6 load-testing}

Grafana k6\footnote{Grafana k6: https://k6.io/}, or k6, is a load-testing tool for assessing the performance of local and remote software systems. It is a script-based tool, where developers use JavaScript to define their testing logic, scenarios, and metrics, and then these tests are run using a Go-based custom JavaScript runtime to minimise test performance overheads.

k6 tests are built around the concept of a ``Virtual User'' or VU, which represents one unit of work that carries out one iteration of the defined test logic. The test logic may carry out a simple action such as making a request to a website at some URL, and waiting for the response. During a test, k6 displays the number of iterations being completed by VUs, as well as the number of VUs that are stalled in waiting for their respective request responses. VUs are run concurrently within a runtime thread pool, with each VU requiring an allocation of memory depending on the work each test iteration does.

Test scenarios can be defined in a number of ways. One test may utilise a ``constant arrival rate'' executor, which attempts to send a fixed number of test iterations for every unit of time (seconds, minutes, etc.) using the available pool of VUs. Another could use a ``ramping arrival rate'' executor, which can be configured to start an increasing, decreasing, or even constant number of test iterations per unit of time, at different stages of the test.

There are several other scenario executors that can be used within k6, but these have the possibility of skewing test results. For example, the ``constant VUs'' executor tries to use a set number of VUs to launch as ``as many iterations as possible''. However, if the executor detects that the time required to complete an iteration increases beyond a certain limit (i.e. the request response time is increasing), it will decrease the iteration start rate, and thereby reduce the number of requests being started. When the goal is to test how a system responds under specific conditions, this style of execution is unfavourable, given that it will dynamically change the test conditions. As such, the executor utilised in testing the Ahuora Digital Platform has been limited to the ramping arrival rate executor, which can model both flat and variable load profiles.

\subsubsection{Active test client and system monitoring}

As performance tests were carried out, the Grafana monitoring tool was used to gain insight into how the tests were impacting the different parts of the cluster, including at the container level, node level, and the overall cluster. Apart from this, it was important to ensure that the intended loads could actually be generated by the client sending the requests to the system. If the client encounters CPU or memory limits when attempting to generate load, then test results will be affected by the performance of the client, rather than the system, and prevent any meaningful insights being obtained. To ensure that client-side test throttling is not taking place, the \verb|htop| Linux command was used to observe client CPU and memory usage. The thresholds of 90\% CPU usage (across all cores) and 80\% memory usage were chosen as causes for concern for test validity.

\subsection{Key metrics}

There are three key metrics that will be used to assess system performance impacts: the HTTP response time, throughput and the error rate. Within the context of self-adaptive systems, as described by \citeauthor{weyns_engineering_2018} \cite{weyns_engineering_2018}, there are two levels of a system that can be optimised. Improvements can be made to the managed system, or to the managing system. For the Ahuora Digital Platform, the optimisation explored here will focus on optimisation of the managing system, or the Kubernetes cluster. These metrics will serve to inform steps towards such optimisation.

\subsubsection{Response time}

The response time for an individual request is the total length of time taken to receive a response from a destination endpoint. This includes the duration of all steps involved in the request, including the time taken to establish an initial connection, perform a TLS handshake, wait for the response, and receive the data. As a metric, it allows an analyst to observe how a system responds over time to increasing load, with a worsening response time indicating that the system is being throttled or is reaching a performance ceiling. From a developer perspective, the response time is important to track, as an increasing response time worsens the experience for users of a product, who may perceive a slowly responding application as a reflection of poor software quality.

Response times will be assessed using an exponentially-weighted moving average (EWMA), along with comparisons at different moving percentiles. Use of a rolling median would appear either too unstable with a small window, or lag behind in showing recent trends. Using an EWMA allows recent data to be paid more attention, and enables better correlation analysis to be conducted.

\subsubsection{Throughput}

The throughput of a system is the number of requests that are processed per second. This metric can be used to identify the peak capacity of the system, as well as identify the thresholds or points at which system performance degrades. As opposed to the response time, which is concerned with the performance of individual requests, throughput is a metric that provides insight only with respect to behaviour of the overall system. One goal within a performance optimisation context is to maximise the peak throughput of a system to serve more users concurrently.

\subsubsection{Error rate}

Along with speed-oriented metrics like the response time and throughput, it is critical to keep track of the error rate, or the proportion of HTTP requests that failed. Error rates can increase under overloaded system conditions, and can provide early indication that a sustainable throughput threshold has been passed. Another reason to record errors is to ensure that they do not cause invalid observations to be made about test data. In some situations, failed requests may have lower response times than their successful counterparts, which may mislead one to think the system is more responsive than it actually is.

\subsection{Secondary metrics}

Both CPU and memory usage will provide additional insight into system performance influences. For example, the observation of high consumption of allocated CPU time within a pod at the same time as request rate instability would possibly indicate the pod is reaching computation limits. These metrics will be recorded at the container level as well as at the node level. 

\section{Performance analysis experiments}
\subsection{Benchmark environment}

A reference point to compare collected test metrics against is needed to obatin meaningful insights with respect to performance improvements. A series of benchmarks were created to compare various Kubernetes cluster configuration impacts against. As one of the purposes of this project is to analyse how transitioning to a cloud-based system can affect the performance of a process engineering simulation application, all benchmarks assess this system running entirely on one system, with all interprocess communication taking place locally.

These benchmarks will run on the ingress node, which does not participate in the Kubernetes cluster, but can access cluster nodes over the network. This local deployment is set up with Docker Compose\footnote{Docker Compose: https://docs.docker.com/compose/}, which allows a set of containerised software components to be configured and run with ease on a local system. Docker configures a private network shared by each container defined in the configuration manifest, and enables DNS-based communication between containers. The manifest used in benchmarks defines four containers: the Django API server, a PostgreSQL database, the PgBouncer database connection pooler, and the IDAES mathematical solver service. The API server uses PgBouncer as its database endpoint, which manages the reuse of connections made to the database. 

\subsection{Experimental environment}

Load tests will be run from the ingress node against the cluster, with traffic directed at the control node serving as the public entrypoint. All tested software components will be run on the K3s Kubernetes cluster. The core software components interacted with are the same as in the benchmark environment, though the Kubernetes environment introduces a central proxy layer through which all API traffic travels through, and is authorised by.

% Show a diagram of the flow of data for both the experimental and benchmark environment %

\subsection{Test scenarios}

Various load profiles are used to assess system performance and dynamics. This allows for observation of whether the system is appropriately tuned or capable of responding to both flat and variable load profiles, as well as handling extended load periods. Two core API endpoints available on the Django API server are used for each test scenario, as described in Table \ref{table:test-api-endpoints}. Software versions used are: Django API 0.0.17, IDAES worker 0.0.10 and Postgres 16.4.

\begin{tabularx}{\textwidth}{|p{0.2\textwidth}|X|p{0.28\textwidth}|}
    \hline
    \textbf{Test type} & \textbf{Endpoint} & \textbf{Description} \\ \hline
    Unit operation retrieval (UOR) & /api/unitops/unitops/?flowsheetOwner=\{id\} & Retrieves all unit operations belonging to a particular flowsheet. \\ \hline

    Flowsheet solving (FS) & /api/solve/idaes/ & Serialises a flowsheet and sends an internal request to the IDAES service to solve the flowsheet's parameters. \\ \hline

    \caption{API endpoints used for system testing}
    \label{table:test-api-endpoints}
\end{tabularx}

The unit operation retrieval endpoint interacts with the Django API server and the PostgreSQL database. The flowsheet solving endpoint interacts with both of these, as well as the IDAES service. The specific load profiles used against either endpoint will differ, as these endpoints have dissimilar baseline response times. The time required to make a single unit operation retrieval request is between 20 and 40 milliseconds, while a solve request may take 500 to 1000 milliseconds.

% Perhaps get a smoke test done at 1 request per second for both endpoints.

Flat load profiles will have initial ramp-up and ramp-down periods between their core profiles to provide corresponding system warm-up and cooldown periods. These periods will be short, fixed at 5\% of the total test duration each. All load profiles will have a 30-second ``graceful stop'' period, which allows requests already in the queue to complete, preventing the processing of requests from the end of one trial affecting the results of the immediately subsequent trial.

To avoid individual tests skewing the overall results, each test scenario will be run three times, with the resulting data averaged across the three trials.

\subsubsection{Pre-trial scenario set up}

Though these experiments do not interact with nor test the Ahuora Digital Platform's user interface, a React-based front-end, it is still necessary to construct a solvable flowsheet that can be used to retrieve unit operations from, and make solve requests. This flowsheet needs to be consistent between test scenarios and their trials. At the same time, the flowsheet needs to be deleted after usage during a trial to avoid an excessive and duplicated presence for users of the live platform when viewing available flowsheets. As such, it was necessary to construct pre and post-trial flowsheet set up and teardown scripts.

These Node.js-based scripts use the existing Django API to create (and delete) a flowsheet, unit operation, and all their corresponding properties and elements of configuration. The flowsheet constructed consists of a single pump with two material streams, one serving as input to the pump, and another as output. The specific steps involved in creating the flowsheet are:

\begin{enumerate}[itemsep=0pt]
    \item Create flowsheet object
    \item Add water compound to flowsheet
    \item Add Helmholtz Water property package to flowsheet
    \item Create pump unit operation
    \item Set pump efficiency to 0.5
    \item Set pump outlet pressure to 200
    \item Set input stream temperature to 80
    \item Set input stream pressure to 100
    \item Set input stream molar flow to 1000
    \item Set input stream vapor fraction to 0
    \item Set input stream water compound amount to 1
\end{enumerate}

\subsubsection{Post-trial metrics collection}

For tests remotely run against the cluster, metrics internally generated and uploaded to Grafana Cloud by the cluster are pulled by a custom k6 test summary generator, obtaining metrics for container CPU, memory, disk and network usage, as well as general cluster metrics reporting pod and container counts. This data is used in conjunction with test results to find associations between test variables and develop a better model of system performance influences.

\subsubsection{Base k6 load test configuration}

\begin{tabularx}{\textwidth}{|p{0.26\textwidth}|p{0.06\textwidth}|X|}
    \hline
    \textbf{k6 option} & \textbf{Value} & \textbf{Usage explanation}  \\ \hline
    insecureSkipTLSVerify & true & The TLS certificate presented to clients by the cluster is self-signed. External traffic from the internet accesses the platform via Cloudflare, which presents its own trusted TLS certificate. Since experimental load tests will be conducted over the local network, the k6 HTTP client by default will fail because of TLS verification failure. The IP address locally associated with the domain on the ingress node can be trusted. \\ \hline
    noConnectionReuse & true & Every newly started request should simulate the action of an independent user, and therefore consume the expected resources in establishing a new connection to the platform backend.  \\ \hline
    discardResponseBodies & true & Since the response body is not being used in test logic, memory consumption by the testing client can be reduced and thereby reduce any undue influence the testing client may have on test results. \\ \hline
    maxRedirects & 0 & It is expected that all requests made will result in a direct response. Any HTTP redirects may indicate, in the case of cluster-bound requests, that the test client is not authenticated and has been redirected to a sign-in page. In any case, redirects would distort test results. Redirects should cause a request iteration to report failure. \\ \hline

    \caption{Base options set for k6 load tests}
    \label{table:k6-load-test-options}
\end{tabularx}
    

\subsubsection{Average-load testing}

This load profile will test the system against a flat load profile, or a fixed number of requests per second. The intent is to assess how the system performs under what can be called an ``average'' load. The Ahuora Digital Platform is not publicly available as of writing, and so the average load will have to be assumed at some value, as we do not have actual usage data that can be used to inform an appropriate average load value. Tests will last ten minutes.


\begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{Test type} & \textbf{Load (requests/second)}  \\ \hline
    UOR & 15 \\ \hline
    FS & 1 \\ \hline

    \caption{Average load values for API endpoints}
    \label{table:test-average-load-plan}
\end{tabularx}

\subsubsection{Stress testing}

The stress testing load profile will assess the system at an ``above-average'' load, which will be set at 200\% of the previously outlined average load values (Table \ref{table:test-average-load-plan}). The goal is to determine what performance degradation (if any) happens when the system experiences load that is still within an expected range.

\begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{Test type} & \textbf{Load (requests/second)}  \\ \hline
    UOR & 30 \\ \hline
    FS & 2 \\ \hline

    \caption{Stress test load values for API endpoints}
    \label{table:test-stress-load-plan}
\end{tabularx}
    

\subsubsection{Spike testing}

Spike tests will test how the system responds to a rapidly increasing, very high load (that may be overwhelming) that then rapidly decreases. These tests will last for four minutes.

\begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{Test type} & \textbf{Load (requests/second)}  \\ \hline
    UOR & 80 \\ \hline
    FS & 8 \\ \hline

    \caption{Spike load values for API endpoints}
    \label{table:test-spike-load-plan}
\end{tabularx}

\subsubsection{Breakpoint testing}

These load profiles will linearly increase the number of requests made against the system per second to some extremely high threshold. This will allow the ``breakpoint'' of the system to be identified, or the point at which system performance either rapidly deterioates or completely collapses. These tests will run for a maximum of ten minutes. To prevent tests for running far beyond the breakpoint unnecessarily, a threshold will be defined for each breakpoint load profile variant, which will mandate that the number of virtual users (VUs) does not exceed 100. If this threshold is exceeded, then the test will cease.

\begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{Test type} & \textbf{Load (requests/second)}  \\ \hline
    UOR & 200 \\ \hline
    FS & 32 \\ \hline

    \caption{Breakpoint load values for API endpoints}
    \label{table:test-breakpoint-load-plan}
\end{tabularx}

\subsection{Horizontal scaling policies}

Within Kubernetes, an operator can define the scaling policies associated with a specific workload. Scaling policies define how many replicas of a workload should be running, based on either static values, or dynamic metrics such as CPU, network or memory usage. As a workload operates, the cluster control plane will monitor the resource utilisation of the workload, and perform scaling operations (both scaling up and down) based on the corresponding scaling policies. The following experiments will assess different scaling policy elements and their impact on the selected key metrics.

The \verb|HorizontalPodAutoscaler| settings used in these tests will have the scale-down stabilisation window set to 30 seconds to allow the autoscaler to reset quickly inbetween tests. The default for this window is 300 seconds, which limits the scaling action instability (pod replicas rapidly increasing and decreasing unnecessarily), as the autoscaler will look at all CPU resource usage within the past 300 seconds and take the maximum utilisation value to inform scaling actions. Setting it to 30 seconds will still provide some stability, but avoid long cooldown periods.

\subsubsection{Replica count}

On a Kubernetes \verb|Deployment| object, the ``replicas'' field statically defines the number of pod replicas that should be running on the cluster. This test will assess how the number of active replicas influences system throughput and response time.

\noindent\textbf{Load profiles used:} Average, stress, spike, breakpoint

\begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{Test iteration} & \textbf{Django replicas}  \\ \hline
    1 & 1 \\ \hline
    2 & 2 \\ \hline
    3 & 4 \\ \hline
    4 & 8 \\ \hline

    \caption{Parameters for UOR replica count tests}
    \label{table:test-replica-count-uor}
\end{tabularx}

\begin{tabularx}{\textwidth}{|X|X|X|}
    \hline
    \textbf{Test iteration} & \textbf{Django replicas} & \textbf{IDAES replicas}  \\ \hline
    1 & 1 & 1 \\ \hline
    2 & 2 & 2 \\ \hline
    3 & 4 & 4 \\ \hline
    4 & 8 & 8 \\ \hline
    5 & 1 & 8 \\ \hline
    6 & 2 & 8 \\ \hline
    7 & 4 & 8 \\ \hline

    \caption{Parameters for FS replica count tests}
    \label{table:test-replica-count-fs}
\end{tabularx}

\subsubsection{Resource allocation and target utilisation}

CPU and memory resource requests are set on deployment manifests for containers within a pod. In this context, each pod has one container. These resource requests indicate to the cluster the minimum amount of CPU time  that needs to be made available to the pod. The pod may not use all of its resource request, but this allows the cluster pod scheduler to decide how to allocate unscheduled pods across the CPU resource pool available. The unit for a CPU resource request is the millicore, which is one thousandth of one physical CPU core.

This metric is used by the cluster to make scaling decisions. Using the CPU resource request usage across all pod replicas within a deployment, an average utilisation percentage is taken, and compared with the target utilisation percentage defined in the corresponding \verb|HorizontalPodAutoscaler| scaling policy. If the average utilisation exceeds the target by a certain threshold, then more pods are scheduled and run to achieve the target. Likewise, utilisation falling below the target by the same threshold will result in pods being removed.

These tests will assess the impact of the permutations of three CPU resource allocation settings and four target utilisation thresholds on response time and throughput. The number of Django API and IDAES replicas over time will be observed.

\noindent\textbf{Load profiles used:} Spike, breakpoint

\begin{tabularx}{\textwidth}{|p{0.15\textwidth}|X|X|}
    \hline
    \textbf{Test iteration} & \textbf{CPU allocation (millicores)} & \textbf{Average CPU utilisation target (\%)}  \\ \hline
    1 & 500 & 25 \\ \hline
    2 & 500 & 50 \\ \hline
    3 & 500 & 75 \\ \hline
    4 & 500 & 90 \\ \hline
    5 & 1000 & 25 \\ \hline
    6 & 1000 & 50 \\ \hline
    7 & 1000 & 75 \\ \hline
    8 & 1000 & 90 \\ \hline
    9 & 2000 & 25 \\ \hline
    10 & 2000 & 50 \\ \hline
    11 & 2000 & 75 \\ \hline
    12 & 2000 & 90 \\ \hline

    \caption{Parameters for resource allocation and target utilisation tests (UOR and FS)}
    \label{table:test-resource-allocation}
\end{tabularx}

\subsubsection{Minimum replica count}

The minimum replica count is a property of the \verb|HorizontalPodAutoscaler|, which defines the scaling policies to be adhered to by the control plane's autoscaling controller. The minimum replica count, as plainly indicated, sets a requirement for the smallest number of pod replicas that are allowed to run, regardless of whether the target resource utilisation threshold is being met.

\noindent\textbf{Load profiles used:} Spike, breakpoint

\begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{Test iteration} & \textbf{Minimum replica count (Django)}  \\ \hline
    1 & 1 \\ \hline
    2 & 2 \\ \hline
    3 & 4 \\ \hline

    \caption{Parameters for UOR minimum replica count tests}
    \label{table:test-min-replica-count-uor}
\end{tabularx}

\begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{Test iteration} & \textbf{Minimum replica count (Django and IDAES)}  \\ \hline
    1 & 1 \\ \hline
    2 & 2 \\ \hline
    3 & 4 \\ \hline

    \caption{Parameters for FS minimum replica count tests}
    \label{table:test-min-replica-count-fs}
\end{tabularx}

\subsubsection{Scale-up pod addition limits}

\noindent\textbf{Load profiles used:} Spike, breakpoint