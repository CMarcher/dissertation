\section{Kubernetes cluster architecture}

\section{Performance analysis experiment components}
\subsection{Controlled environment}

Difficulties were encountered obtaining consistent results when conducting ad hoc performance analysis experiments. Because of the usage of a 4G router for providing internet access to the Raspberry Pi cluster, highly variable bandwidth and latency conditions were encountered. One performance test with increasing load up to sixty requests per second might encounter exponentially increasing response times half-way during the test, while another repetition would see this occur three-quarters through the test. 

Another problem caused by relying on 4G mobile network infrastructure is the upper bandwidth limitation. The 4G router in use has (in its physical position in a lab) a peak download speed of \textasciitilde50.0 Mbit/second (\textasciitilde6.25 Mbyte/second), and an upload speed of \textasciitilde20.0 Mbit/second (\textasciitilde2.5 Mbyte/second). With increasing request rates during performance tests, a bandwidth bottleneck can be encountered, with the cluster unable to serve data because of a lack of sufficient capacity. Though perfect bandwidth and latency stability cannot be expected on the internet, mobile networks are especially prone to instability, and inhibit the ability to replicate a standard environment for applications deployed to ``the cloud'', i.e. a datacentre with high bandwidth capacity. This is exarcerbated by the usage of Cloudflare Tunnel on the cluster to enable external access, which provides a lower bound on maximum bandwidth than if the router was directly accessible via the internet.

To conduct consistent load-tests that provide a higher signal to noise ratio, a stable cloud infrastructure environment was simulated. Load-tests were performed from the ingress node, sending requests to the Kubernetes control node over the LAN (local area network) provided by the network switch. An Auckland-based data centre environment will be replicated, with an expected latency of 5 miiliseconds from an in-country client device to the data centre. This latency will be simulated via the \verb|tc| Linux command as depicted in Listings \ref{lst:pre-test-latency} and \ref{lst:post-test-latency}. The \verb|tc| command allows the traffic travelling via a particular network interface to have artificial constraints added, such as with bandwidth and latency. Prior to every test, 5 milliseconds of latency was added, and then removed after each test.

\begin{center}
    \begin{minipage}{\linewidth}
    \centering
    \begin{lstlisting}[language=bash, caption={Pre-test latency simulation command},label=lst:pre-test-latency]
    
    sudo tc qdisc add dev eth0 root netem delay 5ms
    
    \end{lstlisting}
    \end{minipage}
\end{center}

\begin{center}
    \begin{minipage}{\linewidth}
    \centering
    \begin{lstlisting}[language=bash, caption={Post-test latency simulation teardown},label=lst:post-test-latency]
    
        sudo tc qdisc del dev eth0 root netem
    
    \end{lstlisting}
    \end{minipage}
\end{center}

% ^^ fix stupid centreing and spacing later ^^

% Collect data to demonstrate the instability of external access %

% Show maximum LAN bandwidth results

% Show results of added latency perhaps.

\subsection{Controlling variables}

As with any valid experiment, any one load-test carried out assesses the performance differences observed after modifying one variable. For example, the number of replicas for a Kubernetes deployment, in one set of experiments, have been varied, while all other configuration fields have been kept the same. In some experiments, an entire block of configuration (such as the presence of a \verb|HorizontalPodAutoscaler| in a \verb|Deployment|) have been independently assessed, and everything else controlled. 

\subsection{Preliminary performance modelling}
\subsection{Empirical comparison}
\subsubsection{Grafana k6 load-testing}

Grafana k6\footnote{Grafana k6: https://k6.io/}, or k6, is a load-testing tool for assessing the performance of local and remote software systems. It is a script-based tool, where developers use JavaScript to define their testing logic, scenarios, and metrics, and then these tests are run using a Go-based custom JavaScript runtime to minimise test performance overheads.

k6 tests are built around the concept of a ``Virtual User'' or VU, which represents one unit of work that carries out one iteration of the defined test logic. The test logic may carry out a simple action such as making a request to a website at some URL, and waiting for the response. During a test, k6 displays the number of iterations being completed by VUs, as well as the number of VUs that are stalled in waiting for their respective request responses. VUs are run concurrently within a runtime thread pool, with each VU requiring an allocation of memory depending on the work each test iteration does.

Test scenarios can be defined in a number of ways. One test may utilise a ``constant arrival rate'' executor, which attempts to send a fixed number of test iterations for every unit of time (seconds, minutes, etc.) using the available pool of VUs. Another could use a ``ramping arrival rate'' executor, which can be configured to start an increasing, decreasing, or even constant number of test iterations per unit of time, at different stages of the test.

There are several other scenario executors that can be used within k6, but these have the possibility of skewing test results. For example, the ``constant VUs'' executor tries to use a set number of VUs to launch as ``as many iterations as possible''. However, if the executor detects that the time required to complete an iteration increases beyond a certain limit (i.e. the request response time is increasing), it will decrease the iteration start rate, and thereby reduce the number of requests being started. When the goal is to test how a system responds under specific conditions, this style of execution is unfavourable, given that it will dynamically change the test conditions. As such, the executors utilised in testing the Ahuora Digital Platform have been limited to the constant and ramping arrival rate exectors.

\subsubsection{Active test client and system monitoring}

As performance tests were carried out, the Grafana monitoring tool was used to gain insight into how the tests were impacting the different parts of the cluster, including at the container level, node level, and the overall cluster. Apart from this, it was important to ensure that the intended loads could actually be generated by the client sending the requests to the system. If the client encounters CPU or memory limits when attempting to generate load, then test results will be affected by the performance of the client, rather than the system, and prevent any meaningful insights being obtained. To ensure that client-side test throttling is not taking place, the \verb|htop| Linux command was used to observe client CPU and memory usage. The thresholds of 90\% CPU usage (across all cores) and 80\% memory usage were chosen as causes for concern for test validity.

\subsection{Key metrics}
\subsubsection{Response time}
\subsubsection{Throughput}
\subsubsection{Quantile comparison}
\subsubsection{Error rate}
\subsection{Secondary metrics}
\subsubsection{CPU usage}
\subsubsection{Memory usage}

\section{Performance analysis experiments}
\subsection{Benchmark environment}

A reference point to compare collected test metrics against is needed to obatin meaningful insights with respect to performance improvements. A series of benchmarks were created to compare various Kubernetes cluster configuration impacts against. As one of the purposes of this project is to analyse how transitioning to a cloud-based system can affect the performance of a process engineering simulation application, all benchmarks assess this system running entirely on one system, with all interprocess communication taking place locally.

This local deployment is set up with Docker Compose\footnote{Docker Compose: https://docs.docker.com/compose/}, which allows a set of containerised software components to be configured and run with ease on a local system. Docker configures a private network shared by each container defined in the configuration manifest, and enables DNS-based communication between containers. The manifest used in benchmarks defines four containers: the Django API server, a PostgreSQL database, the PgBouncer database connection pooler, and the IDAES mathematical solver service. The API server uses PgBouncer as its database endpoint, which manages the reuse of connections made to the database. 

\subsection{Test scenarios}

Various load profiles are used to assess system performance and dynamics. This allows for observation of whether the system is appropriately tuned or capable of responding to both flat and variable load profiles, as well as handling extended load periods. Two core API endpoints available on the Django API server are used for each test scenario, as described in Table \ref{table:test-api-endpoints}.

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{|p{0.2\textwidth}|X|p{0.28\textwidth}|}
        \hline
        \textbf{Test type} & \textbf{Endpoint} & \textbf{Description} \\ \hline
        Unit operation retrieval (UOR) & /api/unitops/unitops/?flowsheetOwner=\{id\} & Retrieves all unit operations belonging to a particular flowsheet. \\ \hline

        Flowsheet solving (FS) & /api/solve/idaes/ & Serialises a flowsheet and sends an internal request to the IDAES service to solve the flowsheet's parameters. \\ \hline
    \end{tabularx}
    \caption{API endpoints used for system testing}
    \label{table:test-api-endpoints}
\end{table}

The unit operation retrieval endpoint interacts with the Django API server and the PostgreSQL database. The flowsheet solving endpoint interacts with both of these, as well as the IDAES service. The specific load profiles used against either endpoint will differ, as these endpoints have dissimilar baseline response times. The time required to make a single unit operation retrieval request is between 20 and 40 milliseconds, while a solve request may take 500 to 1000 milliseconds.

% Perhaps get a smoke test done at 1 request per second for both endpoints.

Flat load profiles will have initial ramp-up and ramp-down periods between their core profiles to provide corresponding system warm-up and cooldown periods. These periods will be short, fixed at 5\% of the total test duration each.

\subsubsection{Average-load testing}

This load profile will test the system against a flat load profile, or a fixed number of requests per second. The intent is to assess how the system performs under what can be called an ``average'' load. The Ahuora Digital Platform is not publicly available as of writing, and so the average load will have to be assumed at some value, as we do not have actual usage data that can be used to inform an appropriate average load value. Tests will last ten minutes.

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|}
        \hline
        \textbf{Test type} & \textbf{Load (requests/second)}  \\ \hline
        UOR & 15 \\ \hline
        FS & 1 \\ \hline
    \end{tabularx}
    \caption{Average load values for API endpoints}
    \label{table:test-average-load-plan}
\end{table}

\subsubsection{Stress testing}

The stress testing load profile will assess the system at an ``above-average'' load, which will be set at 200\% of the previously outlined average load values (Table \ref{table:test-average-load-plan}). The goal is to determine what performance degradation (if any) happens when the system experiences load that is still within an expected range.

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|}
        \hline
        \textbf{Test type} & \textbf{Load (requests/second)}  \\ \hline
        UOR & 30 \\ \hline
        FS & 2 \\ \hline
    \end{tabularx}
    \caption{Stress test load values for API endpoints}
    \label{table:test-stress-load-plan}
\end{table}

\subsubsection{Soak testing}

This load profile will generate the same per-second load as the average load tests, but for an extended period of time. These tests will allow for gradual system degradation to be detected better. Soak tests will last sixty minutes.

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|}
        \hline
        \textbf{Test type} & \textbf{Load (requests/second)}  \\ \hline
        UOR & 15 \\ \hline
        FS & 1 \\ \hline
    \end{tabularx}
    \caption{Soak load values for API endpoints}
    \label{table:test-soak-load-plan}
\end{table}

\subsubsection{Spike testing}

Spike tests will test how the system responds to a rapidly increasing, very high load (that may be overwhelming) that then rapidly decreases. These tests will last for four minutes.

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|}
        \hline
        \textbf{Test type} & \textbf{Load (requests/second)}  \\ \hline
        UOR & 80 \\ \hline
        FS & 8 \\ \hline
    \end{tabularx}
    \caption{Spike load values for API endpoints}
    \label{table:test-spike-load-plan}
\end{table}

\subsubsection{Breakpoint testing}

These load profiles will linearly increase the number of requests made against the system per second to some extremely high threshold. This will allow the ``breakpoint'' of the system to be identified, or the point at which system performance either rapidly deterioates or completely collapses. These tests will run for ten minutes.

\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|}
        \hline
        \textbf{Test type} & \textbf{Load (requests/second)}  \\ \hline
        UOR & 200 \\ \hline
        FS & 32 \\ \hline
    \end{tabularx}
    \caption{Breakpoint load values for API endpoints}
    \label{table:test-breakpoint-load-plan}
\end{table}

\subsection{Resource allocation}
\subsection{Horizontal scaling policies}
\subsubsection{Minimum replicas}
\subsubsection{Target resource utilisation}
\subsubsection{Stabilisation window}